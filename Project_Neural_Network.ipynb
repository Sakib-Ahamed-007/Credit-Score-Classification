{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing"
      ],
      "metadata": {
        "id": "sER7jYqvhCFZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bLHwW3_xtAb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders"
      ],
      "metadata": {
        "id": "1lVzDWQlY3RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpBGKLVWQBwV"
      },
      "source": [
        "# Import the packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "import os\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx8aReIox2Gi"
      },
      "source": [
        "# load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/Project/train - train.csv.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/Project/test - test.csv.csv')\n",
        "\n",
        "#Check number of rows and columns in the dataset\n",
        "print(\"The dataset has %d rows and %d columns.\" % df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()\n",
        "df_test.describe()"
      ],
      "metadata": {
        "id": "mgL2kdL1iI2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)\n",
        "df_test.head(10)"
      ],
      "metadata": {
        "id": "Ws_lXQyCiqxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding Missing Values**"
      ],
      "metadata": {
        "id": "tAvYgoh-TGJV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6Mf-fsdyx7C"
      },
      "source": [
        "missing_values = df[pd.isnull(df).any(axis=1)]\n",
        "missing_values\n",
        "\n",
        "missing_values = df_test[pd.isnull(df_test).any(axis=1)]\n",
        "missing_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing unnecessary columns"
      ],
      "metadata": {
        "id": "-OEe0uv5gxna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"ID\", \"Customer_ID\", \"Month\", \"Name\", \"SSN\"], axis=1)\n",
        "df_test = df_test.drop([\"ID\", \"Customer_ID\", \"Month\", \"Name\", \"SSN\"], axis=1)\n"
      ],
      "metadata": {
        "id": "gD7h6PmLQ3Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "unique_values_per_column = {}\n",
        "\n",
        "for column in df.columns:\n",
        "    unique_values = df[column].unique()\n",
        "    unique_values_per_column[column] = unique_values\n",
        "\n",
        "# Display the unique values for each column\n",
        "for column, values in unique_values_per_column.items():\n",
        "    print(f\"Column: {column}\")\n",
        "    print(f\"Unique Values: {values}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "r2MCYumcBNuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[(df['Occupation'] != '_______')]\n",
        "df = df[(df['Credit_Mix'] != '_')]\n",
        "df = df[(df['Payment_of_Min_Amount'] != 'NM')]\n",
        "df = df[(df['Payment_Behaviour'] != '!@9#%8')]\n"
      ],
      "metadata": {
        "id": "n0zvoEysDIFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Category Encoding"
      ],
      "metadata": {
        "id": "Z1UiRUFKg52f"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSkyfYRZ5f9e"
      },
      "source": [
        "# 1 = POOR, 2 = Standard and 3 = GOOD\n",
        "df[\"Credit_Score\"] = df[\"Credit_Score\"].apply(lambda x: 0 if x==\"Poor\" else (1 if x==\"Standard\" else 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSnHH730g8Lc"
      },
      "source": [
        "import category_encoders as ce"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9PJkM2sg-T5"
      },
      "source": [
        "encoder = ce.OrdinalEncoder(cols=[\"Occupation\", \"Num_Bank_Accounts\", \"Num_Credit_Card\", \"Num_of_Loan\", \"Type_of_Loan\", \"Num_of_Delayed_Payment\", \"Num_Credit_Inquiries\", \"Credit_Mix\", \"Credit_History_Age\", \"Payment_of_Min_Amount\", \"Payment_Behaviour\"])\n",
        "\n",
        "df = encoder.fit_transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "LBbaPmCLXdLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = ce.OrdinalEncoder(cols=[\"Occupation\",\"Num_Bank_Accounts\", \"Num_Credit_Card\", \"Num_of_Loan\", \"Type_of_Loan\", \"Num_of_Delayed_Payment\", \"Num_Credit_Inquiries\", \"Credit_Mix\", \"Credit_History_Age\", \"Payment_of_Min_Amount\", \"Payment_Behaviour\"])\n",
        "\n",
        "df_test = encoder.fit_transform(df_test)"
      ],
      "metadata": {
        "id": "91UqUt5-gMtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.info()"
      ],
      "metadata": {
        "id": "s-4CwCEMglRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLtvGIDw3KVG"
      },
      "source": [
        "# check missing values in variables\n",
        "\n",
        "df.isnull().sum()\n",
        "print()\n",
        "df_test.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.replace('_', np.nan, inplace=True)\n",
        "df_test.replace('_', np.nan, inplace=True)\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "df_test = df_test.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "df = df.fillna(df.mean())\n",
        "df_test = df_test.fillna(df_test.mean())"
      ],
      "metadata": {
        "id": "7uj0zAJoyf4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Outliers"
      ],
      "metadata": {
        "id": "3BS7Z6zqIn9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Specify the factor for IQR (e.g., 1.5)\n",
        "iqr_factor = 1.5\n",
        "\n",
        "# Compute the first quartile (Q1) and third quartile (Q3)\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "\n",
        "# Calculate the IQR for each column\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Create a boolean mask for outliers\n",
        "outliers_mask = (df < (Q1 - iqr_factor * IQR)) | (df > (Q3 + iqr_factor * IQR))\n",
        "\n",
        "# Replace outliers with median values column-wise\n",
        "df = df.where(~outliers_mask, df.median(axis=0), axis=1)\n",
        "\n",
        "# Display the resulting DataFrame with outliers replaced by median values\n",
        "print(df)"
      ],
      "metadata": {
        "id": "e2yj4Ds9Ivlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "5miyGtW0Svp6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plVmQT0X8zlH"
      },
      "source": [
        "from random import seed\n",
        "from random import randrange\n",
        "from random import random\n",
        "from csv import reader\n",
        "from math import exp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDPdy5QWXHLu"
      },
      "source": [
        "# Initialize a network\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "\tnetwork = list()\n",
        "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "\tnetwork.append(hidden_layer)\n",
        "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "\tnetwork.append(output_layer)\n",
        "\treturn network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbZVAb6IW-1n"
      },
      "source": [
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "\tfor i in range(len(network)):\n",
        "\t\tinputs = row[:-1]\n",
        "\t\tif i != 0:\n",
        "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "\t\tfor neuron in network[i]:\n",
        "\t\t\tfor j in range(len(inputs)):\n",
        "\t\t\t\tneuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
        "\t\t\tneuron['weights'][-1] -= l_rate * neuron['delta']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sigmoid Activation Function**"
      ],
      "metadata": {
        "id": "ukqj5_yPRNYy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTRtI_bhViRc"
      },
      "source": [
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "\tactivation = weights[-1]\n",
        "\tfor i in range(len(weights)-1):\n",
        "\t\tactivation += weights[i] * inputs[i]\n",
        "\treturn activation\n",
        "\n",
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "\treturn 1.0 / (1.0 + exp(-activation))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Back propagation**"
      ],
      "metadata": {
        "id": "D9drU3j8RY7T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTCxunbTYQ6L"
      },
      "source": [
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "    for epoch in range(n_epoch):\n",
        "        sum_error = 0\n",
        "        for row in train:\n",
        "            outputs = forward_propagate(network, row)\n",
        "            expected = [0 for i in range(n_outputs)]\n",
        "            expected[int(row[-1])] = 1\n",
        "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
        "            backward_propagate_error(network, expected)\n",
        "            update_weights(network, row, l_rate)\n",
        "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Descent Method**"
      ],
      "metadata": {
        "id": "JC_TZPEDRkM-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTaWaVXIbKy1"
      },
      "source": [
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "\tinputs = row\n",
        "\tfor layer in network:\n",
        "\t\tnew_inputs = []\n",
        "\t\tfor neuron in layer:\n",
        "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
        "\t\t\tneuron['output'] = transfer(activation)\n",
        "\t\t\tnew_inputs.append(neuron['output'])\n",
        "\t\tinputs = new_inputs\n",
        "\treturn inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGz4qMMFbebQ"
      },
      "source": [
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "    for epoch in range(n_epoch):\n",
        "        sum_error = 0\n",
        "        for row in train:\n",
        "            outputs = forward_propagate(network, row)\n",
        "            expected = [0 for i in range(n_outputs)]\n",
        "            expected[row[-1]] = 1\n",
        "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
        "            backward_propagate_error(network, expected)\n",
        "            update_weights(network, row, l_rate)\n",
        "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Back Propagation Rules**"
      ],
      "metadata": {
        "id": "ZPqylVcoSB4h"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lABuNDLVZAnr"
      },
      "source": [
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "\tfor i in reversed(range(len(network))):\n",
        "\t\tlayer = network[i]\n",
        "\t\terrors = list()\n",
        "\t\tif i != len(network)-1:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\terror = 0.0\n",
        "\t\t\t\tfor neuron in network[i + 1]:\n",
        "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
        "\t\t\t\terrors.append(error)\n",
        "\t\telse:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\tneuron = layer[j]\n",
        "\t\t\t\terrors.append(neuron['output'] - expected[j])\n",
        "\t\tfor j in range(len(layer)):\n",
        "\t\t\tneuron = layer[j]\n",
        "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3wEvKg3Zobx"
      },
      "source": [
        "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
        "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
        "\tn_inputs = len(train[0]) - 1\n",
        "\tn_outputs = len(set([row[-1] for row in train]))\n",
        "\tnetwork = initialize_network(n_inputs, n_hidden, n_outputs)\n",
        "\ttrain_network(network, train, l_rate, n_epoch, n_outputs)\n",
        "\tpredictions = list()\n",
        "\tfor row in test:\n",
        "\t\tprediction = predict(network, row)\n",
        "\t\tpredictions.append(prediction)\n",
        "\treturn(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-hfkYpJZrTw"
      },
      "source": [
        "# Calculate the derivative of an neuron output\n",
        "def transfer_derivative(output):\n",
        "\treturn output * (1.0 - output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Termination Criteria**"
      ],
      "metadata": {
        "id": "WByP9K7gSRQ_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AstAsKMa1-7"
      },
      "source": [
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor i in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaOeF7xAbvYi"
      },
      "source": [
        "# Make a prediction with a network\n",
        "def predict(network, row):\n",
        "\toutputs = forward_propagate(network, row)\n",
        "\treturn outputs.index(max(outputs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Rate**"
      ],
      "metadata": {
        "id": "yUk6WZWzSgEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df.values.tolist()"
      ],
      "metadata": {
        "id": "3_PbiEX2D9QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting target variable values into integer from float\n",
        "dataset = [[int(value) if i == len(row) - 1 else value for i, value in enumerate(row)] for row in dataset]"
      ],
      "metadata": {
        "id": "m3GRVsSGEFwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "YqB1iFS7Ji2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahU3EJFFUTz4"
      },
      "source": [
        "# Find the min and max values for each column\n",
        "def dataset_minmax(dataset):\n",
        "\tminmax = list()\n",
        "\tstats = [[min(column), max(column)] for column in zip(*dataset)]\n",
        "\treturn stats\n",
        "\n",
        "# Rescale dataset columns to the range 0-1\n",
        "def normalize_dataset(dataset, minmax):\n",
        "\tfor row in dataset:\n",
        "\t\tfor i in range(len(row)-1):\n",
        "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
        "\n",
        "# Rescale dataset columns to the range 0-1\n",
        "def normalize_test_dataset(dataset, minmax):\n",
        "\tfor row in dataset:\n",
        "\t\tfor i in range(len(row)):\n",
        "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dboFAHsNyumZ"
      },
      "source": [
        "# normalize input variables\n",
        "minmax = dataset_minmax(dataset)\n",
        "normalize_dataset(dataset, minmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XW6tNmAZztQ"
      },
      "source": [
        "n_folds = 5\n",
        "l_rate = 0.1\n",
        "n_epoch = 20\n",
        "n_hidden = 21\n",
        "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}