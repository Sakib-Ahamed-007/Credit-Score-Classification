{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sER7jYqvhCFZ"
      },
      "source": [
        "# Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bLHwW3_xtAb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lVzDWQlY3RL"
      },
      "outputs": [],
      "source": [
        "!pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpBGKLVWQBwV"
      },
      "outputs": [],
      "source": [
        "# Import the packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "import os\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx8aReIox2Gi"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/Project/train - train.csv.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets/Project/test - test.csv.csv')\n",
        "\n",
        "#Check number of rows and columns in the dataset\n",
        "print(\"The dataset has %d rows and %d columns.\" % df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgL2kdL1iI2u"
      },
      "outputs": [],
      "source": [
        "df.describe()\n",
        "df_test.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws_lXQyCiqxr"
      },
      "outputs": [],
      "source": [
        "df.head(10)\n",
        "df_test.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAvYgoh-TGJV"
      },
      "source": [
        "**Finding Missing Values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6Mf-fsdyx7C"
      },
      "outputs": [],
      "source": [
        "missing_values = df[pd.isnull(df).any(axis=1)]\n",
        "missing_values\n",
        "\n",
        "missing_values = df_test[pd.isnull(df_test).any(axis=1)]\n",
        "missing_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OEe0uv5gxna"
      },
      "source": [
        "### Removing unnecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD7h6PmLQ3Yp"
      },
      "outputs": [],
      "source": [
        "df = df.drop([\"ID\", \"Customer_ID\", \"Month\", \"Name\", \"SSN\"], axis=1)\n",
        "df_test = df_test.drop([\"ID\", \"Customer_ID\", \"Month\", \"Name\", \"SSN\"], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2MCYumcBNuz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "unique_values_per_column = {}\n",
        "\n",
        "for column in df.columns:\n",
        "    unique_values = df[column].unique()\n",
        "    unique_values_per_column[column] = unique_values\n",
        "\n",
        "# Display the unique values for each column\n",
        "for column, values in unique_values_per_column.items():\n",
        "    print(f\"Column: {column}\")\n",
        "    print(f\"Unique Values: {values}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0zvoEysDIFt"
      },
      "outputs": [],
      "source": [
        "df = df[(df['Occupation'] != '_______')]\n",
        "df = df[(df['Credit_Mix'] != '_')]\n",
        "df = df[(df['Payment_of_Min_Amount'] != 'NM')]\n",
        "df = df[(df['Payment_Behaviour'] != '!@9#%8')]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1UiRUFKg52f"
      },
      "source": [
        "### Category Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSkyfYRZ5f9e"
      },
      "outputs": [],
      "source": [
        "# 1 = POOR, 2 = Standard and 3 = GOOD\n",
        "df[\"Credit_Score\"] = df[\"Credit_Score\"].apply(lambda x: 0 if x==\"Poor\" else (1 if x==\"Standard\" else 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSnHH730g8Lc"
      },
      "outputs": [],
      "source": [
        "import category_encoders as ce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9PJkM2sg-T5"
      },
      "outputs": [],
      "source": [
        "encoder = ce.OrdinalEncoder(cols=[\"Occupation\", \"Num_Bank_Accounts\", \"Num_Credit_Card\", \"Num_of_Loan\", \"Type_of_Loan\", \"Num_of_Delayed_Payment\", \"Num_Credit_Inquiries\", \"Credit_Mix\", \"Credit_History_Age\", \"Payment_of_Min_Amount\", \"Payment_Behaviour\"])\n",
        "\n",
        "df = encoder.fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBbaPmCLXdLL"
      },
      "outputs": [],
      "source": [
        "df.head(10)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91UqUt5-gMtu"
      },
      "outputs": [],
      "source": [
        "encoder = ce.OrdinalEncoder(cols=[\"Occupation\",\"Num_Bank_Accounts\", \"Num_Credit_Card\", \"Num_of_Loan\", \"Type_of_Loan\", \"Num_of_Delayed_Payment\", \"Num_Credit_Inquiries\", \"Credit_Mix\", \"Credit_History_Age\", \"Payment_of_Min_Amount\", \"Payment_Behaviour\"])\n",
        "\n",
        "df_test = encoder.fit_transform(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-4CwCEMglRM"
      },
      "outputs": [],
      "source": [
        "df_test.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLtvGIDw3KVG"
      },
      "outputs": [],
      "source": [
        "# check missing values in variables\n",
        "\n",
        "df.isnull().sum()\n",
        "print()\n",
        "df_test.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uj0zAJoyf4r"
      },
      "outputs": [],
      "source": [
        "df.replace('_', np.nan, inplace=True)\n",
        "df_test.replace('_', np.nan, inplace=True)\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "df_test = df_test.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "df = df.fillna(df.mean())\n",
        "df_test = df_test.fillna(df_test.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BS7Z6zqIn9E"
      },
      "source": [
        "### Removing Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2yj4Ds9Ivlj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Specify the factor for IQR (e.g., 1.5)\n",
        "iqr_factor = 1.5\n",
        "\n",
        "# Compute the first quartile (Q1) and third quartile (Q3)\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "\n",
        "# Calculate the IQR for each column\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Create a boolean mask for outliers\n",
        "outliers_mask = (df < (Q1 - iqr_factor * IQR)) | (df > (Q3 + iqr_factor * IQR))\n",
        "\n",
        "# Replace outliers with median values column-wise\n",
        "df = df.where(~outliers_mask, df.median(axis=0), axis=1)\n",
        "\n",
        "# Display the resulting DataFrame with outliers replaced by median values\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "AYPwN3d0cUdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determining Feature Importance with Random Forest Classifier"
      ],
      "metadata": {
        "id": "K8611IfSeM_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming you have a DataFrame 'df' with features and a target variable\n",
        "# Replace this with your actual DataFrame\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop(\"Credit_Score\", axis=1)  # Assuming 'target' is the column you want to predict\n",
        "y = df[\"Credit_Score\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (not always necessary for Random Forest, but can be useful)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Get feature importances from the trained model\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Display feature importances in a bar plot\n",
        "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Random Forest Feature Importance')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wru6AcrCjoAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC AUC Curve"
      ],
      "metadata": {
        "id": "Vogfdfhxf2hH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IGfAQSQKnKM"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score,\n",
        "                             classification_report, f1_score, average_precision_score, precision_recall_fscore_support)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have a DataFrame 'df' with features and a target variable\n",
        "# Replace this with your actual DataFrame\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop(\"Credit_Score\", axis=1)  # Assuming 'target' is the column you want to predict\n",
        "y = df[\"Credit_Score\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (not always necessary for Decision Tree, Random Forest, and Neural Network)\n",
        "# Comment out these lines if not needed\n",
        "scaler = StandardScaler()\n",
        "train_X_scaled = scaler.fit_transform(train_X)\n",
        "test_X_scaled = scaler.transform(test_X)\n",
        "\n",
        "# Decision Tree\n",
        "modelDT = DecisionTreeClassifier(random_state=0)\n",
        "y_score_dt = modelDT.fit(train_X_scaled, train_Y).predict_proba(test_X_scaled)\n",
        "\n",
        "# Neural Network\n",
        "modelNN = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=0)\n",
        "y_score_nn = modelNN.fit(train_X_scaled, train_Y).predict_proba(test_X_scaled)\n",
        "\n",
        "# Random Forest\n",
        "modelRF = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "y_score_rf = modelRF.fit(train_X_scaled, train_Y).predict_proba(test_X_scaled)\n",
        "\n",
        "# Determine the number of classes\n",
        "num_classes = test_Y.shape[1] if len(test_Y.shape) > 1 else len(np.unique(test_Y))\n",
        "\n",
        "# Convert DataFrame to NumPy array if necessary\n",
        "if isinstance(test_Y, pd.DataFrame):\n",
        "    test_Y_array = test_Y.values\n",
        "else:\n",
        "    test_Y_array = np.asarray(test_Y)\n",
        "\n",
        "# Compute ROC curves and AUCs for each model\n",
        "models = [\n",
        "    ('Decision Tree', y_score_dt),\n",
        "    ('Neural Network', y_score_nn),\n",
        "    ('Random Forest', y_score_rf)\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for model_name, y_score_model in models:\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        true_labels = test_Y_array[:, i] if len(test_Y_array.shape) > 1 else (test_Y_array == i).astype(int)\n",
        "        y_score_i = y_score_model[:, i]\n",
        "\n",
        "        fpr[i], tpr[i], _ = roc_curve(true_labels, y_score_i)\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # Plot ROC curves for each model\n",
        "        plt.plot(fpr[i], tpr[i], label=f'{model_name} - Class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "# Plot the random classifier line\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves for Multiclass Classification')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wonz6XfQwAQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Precision-Recall Curve**"
      ],
      "metadata": {
        "id": "iwLpQR4tTkuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for model_name, y_score_model in models:\n",
        "    precision = dict()\n",
        "    recall = dict()\n",
        "    average_precision = dict()\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        true_labels = test_Y_array[:, i] if len(test_Y_array.shape) > 1 else (test_Y_array == i).astype(int)\n",
        "        y_score_i = y_score_model[:, i]\n",
        "\n",
        "        precision[i], recall[i], _ = precision_recall_curve(true_labels, y_score_i)\n",
        "        average_precision[i] = average_precision_score(true_labels, y_score_i)\n",
        "\n",
        "        # Plot precision-recall curves for each model\n",
        "        plt.plot(recall[i], precision[i], label=f'{model_name} - Class {i} (Avg Precision = {average_precision[i]:.2f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curves for Multiclass Classification')\n",
        "plt.legend(fontsize=8)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gi4Lp_1xSK-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification Report**"
      ],
      "metadata": {
        "id": "YLHcV-c6aHil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "for model_name, y_score_model in models:\n",
        "    for i in range(num_classes):\n",
        "        true_labels = test_Y_array[:, i] if len(test_Y_array.shape) > 1 else (test_Y_array == i).astype(int)\n",
        "        y_score_i = y_score_model[:, i]\n",
        "\n",
        "        # Convert probabilities to class predictions (assuming threshold of 0.5, adjust as needed)\n",
        "        y_pred_i = (y_score_i > 0.5).astype(int)\n",
        "\n",
        "        # Print precision, recall, and F1-score for each class\n",
        "        print(f'{model_name} - Class {i}:')\n",
        "        print(f'Precision: {precision_score(true_labels, y_pred_i)}')\n",
        "        print(f'Recall: {recall_score(true_labels, y_pred_i)}')\n",
        "        print(f'F1-score: {f1_score(true_labels, y_pred_i)}\\n')\n",
        "\n",
        "# You can also compute and print a micro-average or macro-average\n",
        "# For example, to compute the macro-average:\n",
        "y_pred_all = np.argmax(np.array([y_score_model for _, y_score_model in models]), axis=2).T\n",
        "true_labels_all = np.argmax(test_Y_array, axis=1) if len(test_Y_array.shape) > 1 else test_Y_array\n"
      ],
      "metadata": {
        "id": "BdiNwMxuU28G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# ... (previous code)\n",
        "\n",
        "# Lists to store results for each model\n",
        "model_names = []\n",
        "class_precision = []\n",
        "class_recall = []\n",
        "class_f1 = []\n",
        "\n",
        "# Loop through models\n",
        "for model_name, y_score_model in models:\n",
        "    model_names.append(model_name)\n",
        "\n",
        "    class_precision_model = []\n",
        "    class_recall_model = []\n",
        "    class_f1_model = []\n",
        "\n",
        "    # Loop through classes\n",
        "    for i in range(num_classes):\n",
        "        true_labels = test_Y_array[:, i] if len(test_Y_array.shape) > 1 else (test_Y_array == i).astype(int)\n",
        "        y_score_i = y_score_model[:, i]\n",
        "\n",
        "        # Convert probabilities to class predictions (assuming threshold of 0.5, adjust as needed)\n",
        "        y_pred_i = (y_score_i > 0.5).astype(int)\n",
        "\n",
        "        # Calculate precision, recall, and F1-score for each class\n",
        "        precision = precision_score(true_labels, y_pred_i)\n",
        "        recall = recall_score(true_labels, y_pred_i)\n",
        "        f1 = f1_score(true_labels, y_pred_i)\n",
        "\n",
        "        class_precision_model.append(precision)\n",
        "        class_recall_model.append(recall)\n",
        "        class_f1_model.append(f1)\n",
        "\n",
        "        # Print precision, recall, and F1-score for each class\n",
        "        print(f'{model_name} - Class {i}:')\n",
        "        print(f'Precision: {precision}')\n",
        "        print(f'Recall: {recall}')\n",
        "        print(f'F1-score: {f1}\\n')\n",
        "\n",
        "    class_precision.append(class_precision_model)\n",
        "    class_recall.append(class_recall_model)\n",
        "    class_f1.append(class_f1_model)\n",
        "\n",
        "# Plotting results\n",
        "fig, axs = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n",
        "x_labels = [f\"Class {i}\" for i in range(num_classes)]\n",
        "\n",
        "# Plot class-wise precision\n",
        "for i, (model_name, precision_values) in enumerate(zip(model_names, class_precision)):\n",
        "    axs[0].plot(x_labels, precision_values, label=model_name)\n",
        "axs[0].set_title('Class-wise Precision')\n",
        "axs[0].legend()\n",
        "\n",
        "# Plot class-wise recall\n",
        "for i, (model_name, recall_values) in enumerate(zip(model_names, class_recall)):\n",
        "    axs[1].plot(x_labels, recall_values, label=model_name)\n",
        "axs[1].set_title('Class-wise Recall')\n",
        "axs[1].legend()\n",
        "\n",
        "# Plot class-wise F1-score\n",
        "for i, (model_name, f1_values) in enumerate(zip(model_names, class_f1)):\n",
        "    axs[2].plot(x_labels, f1_values, label=model_name)\n",
        "axs[2].set_title('Class-wise F1-score')\n",
        "axs[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qiqDlk1NZ898"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting with Random Forest Classifier"
      ],
      "metadata": {
        "id": "NHqfzgpowl8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming your target variable is named 'target' and the features are all other columns\n",
        "# Replace 'target' with your actual target variable name\n",
        "\n",
        "# Split the training dataset into features and target variable\n",
        "X_train = df.drop('Credit_Score', axis=1)\n",
        "y_train = df['Credit_Score']\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set to evaluate the model\n",
        "y_val_pred = rf_classifier.predict(X_val)\n",
        "\n",
        "# Print accuracy on the validation set\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Now, use the trained model to predict the test dataset\n",
        "X_test = df_test  # Assuming df_test contains the features for the test dataset\n",
        "y_test_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# The predictions for the test dataset are stored in y_test_pred variable\n",
        "# You can further use or analyze these predictions as needed\n"
      ],
      "metadata": {
        "id": "Qe8Y0WFrwksF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred"
      ],
      "metadata": {
        "id": "pZXP-SFEw2_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_df_test = df_test.copy()"
      ],
      "metadata": {
        "id": "ICRsb9D5xI2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_df_test[\"Credit_Score\"] = y_test_pred"
      ],
      "metadata": {
        "id": "JOC3thjfxTRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_df_test.head(10)"
      ],
      "metadata": {
        "id": "dDCIshJ5xdGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM-mc84n0YOS"
      },
      "outputs": [],
      "source": [
        "# 1 = POOR, 2 = Standard and 3 = GOOD\n",
        "predicted_df_test[\"Credit_Score\"] = predicted_df_test[\"Credit_Score\"].apply(lambda x: \"Poor\" if x==1 else (\"Standard\" if x==2 else \"Good\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_df_test.head(10)"
      ],
      "metadata": {
        "id": "vgg6wurL0ra6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}